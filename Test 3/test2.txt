["Python3 网络爬虫（二）：下载小说的正确姿势 2020年4月21日08:39:15 61 11,110 °C 摘要小说下载实战讲解，网络爬虫的三个步骤：发起请求、解析数据、保存数据。一、前言网路爬虫，一般爬取的东西无非也就四种：文字、图片、音乐、视频。这是明面上，能想到的东西，除了这些还有一些危险的操作，容易被请喝茶的，就不讨论了。咱们循序渐进，先谈谈如何下载文字内容。PS：文中出现的所有代码，均可在我的 Github 上下载：点击查看二、诡秘之主说到下载文字内容，第一个想到的就是下载小说了。在曾经的以《一念永恒》小说为例进行讲解的 CSDN 文章中，有网友留言道：那么，今天我就再安利一本小说《诡秘之主》。起点中文网，它的月票基本是月月第一。这篇文章其实是在教大家如何白嫖，不过有能力支持正版的朋友，还是可以去起点中文网，支持一下作者的，毕竟创作不易。三、准备工作话不多说，直接进入我们今天的正题，网络小说下载。1、背景介绍小说网站，“新笔趣阁”：https://www.xsbiquge.com/盗版小说网站有很多，曾经爬过“笔趣看”，这回咱换一家，爬“新笔趣阁”，雨露均沾嘛！“新笔趣阁”只支持在线浏览，不支持小说打包下载。本次实战就教大家如何“优雅”的下载一篇名为《诡秘之主》的网络小说。2、爬虫步骤要想把大象装冰箱，总共分几步？要想爬取数据，总共分几步？爬虫其实很简单，可以大致分为三个步骤：发起请求：我们需要先明确如何发起 HTTP 请求，获取到数据。解析数据：获取到的数据乱七八糟的，我们需要提取出我们想要的数据。保存数据：将我们想要的数据，保存下载。发起请求，我们就用 requests 就行，上篇文章已经介绍过。解析数据工具有很多，比如xpath、Beautiful Soup、正则表达式等。本文就用一个简单的经典小工具，Beautiful Soup来解析数据。保存数据，就是常规的文本保存。3、Beautiful Soup简单来说，Beautiful Soup 是 Python 的一个第三方库，主要帮助我们解析网页数据。在使用这个工具前，我们需要先安装，在 cmd 中，使用 pip 或 easy_install 安装即可。Shell\npip install beautifulsoup4\n# 或者\neasy_install beautifulsoup4123pip install beautifulsoup4# 或者easy_install beautifulsoup4安装好后，我们还需要安装 lxml，这是解析 HTML 需要用到的依赖：Shell\npip install lxml1pip install lxmlBeautiful Soup 的使用方法也很简单，可以看下我在 CSDN 的讲解或者官方教程学习，详细的使用方法：我的 Beautiful Soup 讲解：点击查看官方中文教程：点击查看四、小试牛刀我们先看下《诡秘之主》小说的第一章内容。URL：https://www.xsbiquge.com/15_15338/8549128.html我们先用已经学到的知识获取 HTML 信息试一试，编写代码如下：Python\nimport requests\n\nif __name__ == '__main__':\n    target = 'https://www.xsbiquge.com/15_15338/8549128.html'\n    req = requests.get(url = target)\n    req.encoding = 'utf-8'\n    print(req.text)1234567import requests\xa0if __name__ == '__main__':", "target = 'https://www.xsbiquge.com/15_15338/8549128.html'", 'req = requests.get(url = target)', "req.encoding = 'utf-8'", 'print(req.text)这也就是爬虫的第一步“发起请求”，得到的结果如下：可以看到，我们很轻松地获取了 HTML 信息，里面有我们想要的小说正文内容，但是也包含了一些其他内容，我们并不关心 div 、br 这些 HTML 标签。如何把正文内容从这些众多的 HTML 标签中提取出来呢？这就需要爬虫的第二部“解析数据”，也就是使用 Beautiful Soup 进行解析。现在，我们使用上篇文章讲解的审查元素方法，查看一下我们的目标页面，你会看到如下内容：不难发现，文章的所有内容都放在了一个名为div的“东西下面”，这个"东西"就是 HTML 标签。HTML 标签是 HTML 语言中最基本的单位，HTML 标签是 HTML 最重要的组成部分。不理解，没关系，我们再举个简单的例子：一个女人的包包里，会有很多东西，她们会根据自己的习惯将自己的东西进行分类放好。镜子和口红这些会经常用到的东西，会归放到容易拿到的外侧口袋里。那些不经常用到，需要注意安全存放的证件会放到不容易拿到的里侧口袋里。HTML 标签就像一个个“口袋”，每个“口袋”都有自己的特定功能，负责存放不同的内容。显然，上述例子中的 div 标签下存放了我们关心的正文内容。这个 div 标签是这样的：XHTML\n<div id="content" style="font-size: 10pt;">1<div id="content" style="font-size: 10pt;">细心的朋友可能已经发现，除了 div 字样外，还有 id 。id 就是 div 标签的属性，content是属性值，一个属性对应一个属性值。属性有什么用？它是用来区分不同的 div 标签的，因为 div 标签可以有很多，id 可以理解为这个 div 的身份。这个 id 属性为 content 的 div 标签里，存放的就是我们想要的内容，我们可以利用这一点，使用Beautiful Soup 提取我们想要的正文内容，编写代码如下：Python\nimport requests\nfrom bs4 import BeautifulSoup\n\nif __name__ == \'__main__\':\n    target = \'https://www.xsbiquge.com/15_15338/8549128.html\'\n    req = requests.get(url = target)\n    req.encoding = \'utf-8\'\n    html = req.text\n    bs = BeautifulSoup(html, \'lxml\')\n    texts = bs.find(\'div\', id=\'content\')\n    print(texts)1234567891011import requestsfrom bs4 import BeautifulSoup\xa0if __name__ == \'__main__\':', "target = 'https://www.xsbiquge.com/15_15338/8549128.html'", 'req = requests.get(url = target)', "req.encoding = 'utf-8'", 'html = req.text', "bs = BeautifulSoup(html, 'lxml')", "texts = bs.find('div', id='content')", "print(texts)代码很简单，bf.find('div', id='content') 的意思就是，找到 id 属性为 content 的 div 标签。可以看到，正文内容已经顺利提取，但是里面还有一些 div 和 br 这类标签，我们需要进一步清洗数据。Python\nimport requests\nfrom bs4 import BeautifulSoup\n\nif __name__ == '__main__':\n    target = 'https://www.xsbiquge.com/15_15338/8549128.html'\n    req = requests.get(url = target)\n    req.encoding = 'utf-8'\n    html = req.text\n    bs = BeautifulSoup(html, 'lxml')\n    texts = bs.find('div', id='content')\n    print(texts.text.strip().split('\\xa0'*4))1234567891011import requestsfrom bs4 import BeautifulSoup\xa0if __name__ == '__main__':", "target = 'https://www.xsbiquge.com/15_15338/8549128.html'", 'req = requests.get(url = target)', "req.encoding = 'utf-8'", 'html = req.text', "bs = BeautifulSoup(html, 'lxml')", "texts = bs.find('div', id='content')", "print(texts.text.strip().split('\\xa0'*4))texts.text 是提取所有文字，然后再使用 strip 方法去掉回车，最后使用 split 方法根据 \\xa0 切分数据，因为每一段的开头，都有四个空格。程序运行结果如下：所有的内容，已经清洗干净，保存到一个列表里了。小说正文，已经顺利获取到了。要想下载整本小说，我们就要获取每个章节的链接。我们先分析下小说目录：URL：https://www.xsbiquge.com/15_15338/审查元素后，我们不难发现，所有的章节信息，都存放到了 id 属性为 list 的 div 标签下的 a 标签内，编写如下代码：Python\nimport requests\nfrom bs4 import BeautifulSoup\n\nif __name__ == '__main__':\n    target = 'https://www.xsbiquge.com/15_15338/'\n    req = requests.get(url = target)\n    req.encoding = 'utf-8'\n    html = req.text\n    bs = BeautifulSoup(html, 'lxml')\n    chapters = bs.find('div', id='list')\n    chapters = chapters.find_all('a')\n    for chapter in chapters:\n        print(chapter)12345678910111213import requestsfrom bs4 import BeautifulSoup\xa0if __name__ == '__main__':", "target = 'https://www.xsbiquge.com/15_15338/'", 'req = requests.get(url = target)', "req.encoding = 'utf-8'", 'html = req.text', "bs = BeautifulSoup(html, 'lxml')", "chapters = bs.find('div', id='list')", "chapters = chapters.find_all('a')", 'for chapter in chapters:', '', "print(chapter)bf.find('div', id='list') 就是找到 id 属性为 list 的 div 标签，chapters.find_all('a') 就是在找到的 div 标签里，再提取出所有 a 标签，运行结果如下：可以看到章节链接和章节名我们已经提取出来，但是还需要进一步解析，编写如下代码：Python\nimport requests\nfrom bs4 import BeautifulSoup\n\nif __name__ == '__main__':\n    server = 'https://www.xsbiquge.com'\n    target = 'https://www.xsbiquge.com/15_15338/'\n    req = requests.get(url = target)\n    req.encoding = 'utf-8'\n    html = req.text\n    bs = BeautifulSoup(html, 'lxml')\n    chapters = bs.find('div', id='list')\n    chapters = chapters.find_all('a')\n    for chapter in chapters:\n        url = chapter.get('href')\n        print(chapter.string)\n        print(server + url)12345678910111213141516import requestsfrom bs4 import BeautifulSoup\xa0if __name__ == '__main__':", "server = 'https://www.xsbiquge.com'", "target = 'https://www.xsbiquge.com/15_15338/'", 'req = requests.get(url = target)', "req.encoding = 'utf-8'", 'html = req.text', "bs = BeautifulSoup(html, 'lxml')", "chapters = bs.find('div', id='list')", "chapters = chapters.find_all('a')", 'for chapter in chapters:', '', "url = chapter.get('href')", '', 'print(chapter.string)', '', "print(server + url)可以看到，chapter.get('href') 方法提取了 href 属性，并拼接出章节的 url，使用 chapter.string 方法提取了章节名。每个章节的链接、章节名、章节内容都有了。接下来就是整合代码，将内容保存到txt中即可。编写代码如下：Python\nimport requests\nimport time\nfrom tqdm import tqdm\nfrom bs4 import BeautifulSoup\n\ndef get_content(target):\n    req = requests.get(url = target)\n    req.encoding = 'utf-8'\n    html = req.text\n    bf = BeautifulSoup(html, 'lxml')\n    texts = bf.find('div', id='content')\n    content = texts.text.strip().split('\\xa0'*4)\n    return content\n\nif __name__ == '__main__':\n    server = 'https://www.xsbiquge.com'\n    book_name = '诡秘之主.txt'\n    target = 'https://www.xsbiquge.com/15_15338/'\n    req = requests.get(url = target)\n    req.encoding = 'utf-8'\n    html = req.text\n    chapter_bs = BeautifulSoup(html, 'lxml')\n    chapters = chapter_bs.find('div', id='list')\n    chapters = chapters.find_all('a')\n    for chapter in tqdm(chapters):\n        chapter_name = chapter.string\n        url = server + chapter.get('href')\n        content = get_content(url)\n        with open(book_name, 'a', encoding='utf-8') as f:\n            f.write(chapter_name)\n            f.write('\\n')\n            f.write('\\n'.join(content))\n            f.write('\\n')123456789101112131415161718192021222324252627282930313233import requestsimport timefrom tqdm import tqdmfrom bs4 import BeautifulSoup\xa0def get_content(target):", 'req = requests.get(url = target)', "req.encoding = 'utf-8'", 'html = req.text', "bf = BeautifulSoup(html, 'lxml')", "texts = bf.find('div', id='content')", "content = texts.text.strip().split('\\xa0'*4)", "return content\xa0if __name__ == '__main__':", "server = 'https://www.xsbiquge.com'", "book_name = '诡秘之主.txt'", "target = 'https://www.xsbiquge.com/15_15338/'", 'req = requests.get(url = target)', "req.encoding = 'utf-8'", 'html = req.text', "chapter_bs = BeautifulSoup(html, 'lxml')", "chapters = chapter_bs.find('div', id='list')", "chapters = chapters.find_all('a')", 'for chapter in tqdm(chapters):', '', 'chapter_name = chapter.string', '', "url = server + chapter.get('href')", '', 'content = get_content(url)', '', "with open(book_name, 'a', encoding='utf-8') as f:", '', '', 'f.write(chapter_name)', '', '', "f.write('\\n')", '', '', "f.write('\\n'.join(content))", '', '', "f.write('\\n')下载过程中，我们使用了 tqdm 显示下载进度，让下载更加“优雅”，如果没有安装 tqdm，可以使用 pip 进行安装，运行效果：可以看到，小说内容保存到“诡秘之主.txt”中，小说一共 1416 章，下载需要大约 20 分钟，每秒钟大约下载 1 个章节。下载完成，实际花费了 27 分钟。20 多分钟下载一本小说，你可能感觉太慢了。想提速，可以使用多进程，大幅提高下载速度。如果使用分布式，甚至可以1秒钟内下载完毕。但是，我不建议这样做。我们要做一个友好的爬虫，如果我们去提速，那么我们访问的服务器也会面临更大的压力。以我们这次下载小说的代码为例，每秒钟下载 1 个章节，服务器承受的压力大约 1qps，意思就是，一秒钟请求一次。如果我们 1 秒同时下载 1416 个章节，那么服务器将承受大约 1416 qps 的压力，这还是仅仅你发出的并发请求数，再算上其他的用户的请求，并发量可能更多。如果服务器资源不足，这个并发量足以一瞬间将服务器“打死”，特别是一些小网站，都很脆弱。过大并发量的爬虫程序，相当于发起了一次 CC 攻击，并不是所有网站都能承受百万级别并发量的。所以，写爬虫，一定要谨慎，勿给服务器增加过多的压力，满足我们的获取数据的需求，这就够了。你好，我也好，大家好才是真的好。五、总结本文讲解了网络爬虫的三个步骤：发起请求、解析数据、保存数据。注意并发量，勿给服务器带来过多的压力。参考链接（我参考我自己）：https://blog.csdn.net/c406495762/article/details/78123502https://blog.csdn.net/c406495762/article/details/71158264 微信公众号分享技术，乐享生活：微信公众号搜索「JackCui-AI」关注一个在互联网摸爬滚打的潜行者。  赞  58    赏  分享  A+所属分类：网络爬虫Python3网络爬虫网络爬虫教程小说下载 版权声明：本站原创文章，于2020年4月21日08:39:15，由 Jack Cui 发表，共 3326 字。转载请注明：Python3 网络爬虫（二）：下载小说的正确姿势 | Jack Cui 宝藏B站UP主，视频弹幕尽收囊中！ Python3 网络爬虫（六）：618，爱他/她，就清空他/她的购物车！ Python3 网络爬虫（五）：老板，需要特殊服务吗？ Python3 网络爬虫（四）：视频下载，那些事儿！相关文章 宝藏B站UP主，视频弹幕尽收囊中！ Python3 网络爬虫（六）：618，爱他/她，就清空他/她的购物车！ Python3 网络爬虫（五）：老板，需要特殊服务吗？最新文章 2022年高能AI算法，只有想不到，没有做不到！ 万万没想到，英伟达还有这一招… AI艺术家，DALL·E 2来了！  上一篇Python3 网络爬虫（一）：初识网络爬虫之夜探老王家下一篇 Python3 网络爬虫（三）：漫画下载，动态加载、反爬虫这都不叫事！文章导航发表评论取消回复                         昵称*  邮箱*  网址  QQ      目前评论：61 \xa0\xa0其中：访客\xa0\xa043 \xa0\xa0博主\xa0\xa018   blackwolfxxx     河南省开封市 联通 0    回复 2020年11月4日 下午5:08  \xa021楼   合并代码 有点重复的感觉！！！   咸鱼     安徽省合肥市 安徽农业大学 0    回复 2020年11月23日 下午8:35  \xa01层   @blackwolfxxx def get_bf(target): req = requests.get(url=target) req.encoding = ‘utf-8’ html = req.text bf = BeautifulSoup(html, ‘lxml’) return bfdef get_chapters(target): chapters = get_bf(target).find(‘div’, id=’list’) chapters = chapters.find_all(‘a’) return chaptersdef get_content(target): texts = get_bf(target).find(‘div’, id=’content’) content = texts.text.strip().split(‘nbsp;’ * 4) return content我把站主的代码简单的封装了一下   Jack Cui  Admin    北京市 电信    回复 2020年11月24日 下午11:15  \xa02层   @咸鱼 漂亮！   基神     广东省广州市 联通 0    回复 2020年11月7日 下午1:10  \xa022楼   怎么我爬的时候不能把id一样的标签全部爬完   刘民     江苏省常州市 移动 0    回复 2021年5月11日 下午3:02  \xa023楼   楼主，请问出现“0%|          | 0/254 [00:00<?, ?it/s]”这种情况怎么回事啊   温水     四川省南充市 联通 1    回复 2021年5月24日 下午10:20  \xa024楼   f.write(‘\\n’.join(content))什么意思啊，不太懂，求解答一下   温水     四川省南充市 联通 1    回复 2021年5月24日 下午10:24  \xa025楼   而且出现这个：由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。’))   HC     江苏省南京市 电信 0    回复 2021年5月26日 下午5:16  \xa01层   @温水 因为本身网页就已经无法打开了吧   pypy     湖南省 电信 1    回复 2021年5月30日 下午5:36  \xa026楼   您好！print(texts.text.strip().split(‘\\xa0’*4))去除的原理是什么？   Jack Cui  Admin    北京市 电信    回复 2021年5月30日 下午6:14  \xa01层   @pypy 就是去掉这些空格了。   pypy     湖南省 电信 1    回复 2021年5月30日 下午6:23  \xa02层   @Jack Cui 知道了谢谢   pypy     湖南省 电信 1    回复 2021年5月30日 下午6:30  \xa02层   @Jack Cui 那些是怎么清除的呢?   LYM     广东省 移动 1    回复 2022年1月22日 下午9:33  \xa027楼   太棒了，只不过我弄的目录标签是dl，坑爹，还带class，还以为没有div弄不了呢   LYM     广东省 移动 1    回复 2022年1月22日 下午10:08  \xa01层   @LYM 网站是gbk的，encoding  gbk，下载出问题，encoding  utf-8  乱码…………   LYM     广东省 移动 1    回复 2022年1月22日 下午10:18  \xa01层   @LYM 我服了，gbk和GBK没区别吧，怎么gbk就出错，GBK一次过   LYM     广东省 移动 1    回复 2022年1月23日 下午9:23  \xa01层   @LYM 爬虫真是充满了不确定性明明根据网站信息改了代码，却总是中途失败 content = page.text.strip().split(‘\\xa0’ * 4) AttributeError: ‘NoneType’ object has no attribute ‘text’ 有一次竟发现有一章的一段不止开头的4个空格，在文中还有2个，还在不同地方评论导航较早评论评论导航 第 1 页 第 2 页 第 3 页关于本站 网站宗旨把最实用的经验，分享给最需要的读者，希望每一位来访的朋友都能有所收获！  文章 299 留言 5140相关文章 宝藏B站UP主，视频弹幕尽收囊中！ Python3 网络爬虫（六）：618，爱他/她，就清空他/她的购物车！ Python3 网络爬虫（五）：老板，需要特殊服务吗？ Python3 网络爬虫（四）：视频下载，那些事儿！ Python3 网络爬虫（三）：漫画下载，动态加载、反爬虫这都不叫事！微信公众号 推荐栏目视频教程机器学习深度学习网络爬虫算法学习大厂面经程序生活网站建设您曾看过"]